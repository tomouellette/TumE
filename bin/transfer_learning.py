#!/usr/bin/env python3
import numpy as np
import pandas as pd
import argparse
import os
import torch
from functools import partial
import string
import random
from nets import *
from utils import *

"""
Training deep learning models using pre-trained TumE models for inference of parameters generated by TEMULATOR simulations
"""

def arg_parse():
	parser = argparse.ArgumentParser()
	parser.add_argument('-p', '--path', type=str, help='Path to directory containing features and labels from evolutionary simulations (TEMULATOR)')
	parser.add_argument('-md', '--model_dir', type=str, help='Directory containing pre-trained models')
	parser.add_argument('-m1', '--model1', type=str, help='First pretrained model of kind == evolution')
	parser.add_argument('-m2', '--model2', type=str, help='Second pretrained model of kind == onesubclone')
	parser.add_argument('-od', '--outputdir', type=str, help='Output directory path to save files')
	args = parser.parse_args()
	return args

def random_hyperparameters():
	"""
	Generates a random sample of hyperparameters for training multi-task, multi-convolution networks
	"""
	n_linear = np.random.randint(1, 8)
	lr = float(10**-np.random.default_rng().uniform(3, 7))	
	gradients = np.random.choice([True, False])
	pre_trained = np.random.choice([True, False])
	return n_linear, lr, gradients, pre_trained

def train_temulator(pretrained_models, n_linear:int, lr: float, gradients: bool, directory: str, input_dim = 192, epochs = 10) -> tuple:
	"""
	Training function for building models to predict evolutionary mode and the number of subclones
	"""
	model = TransferModel(pretrained_models, n_linear = n_linear, gradients = gradients, input_dim = input_dim, n_tasks = 4)
	net = EvoNet(model, task_types = ['r', 'r', 'r', 'r'])	
	mod, dt = net.iterativefit(directory, temulator_loader, batchsize = 256, epochs = epochs, optimizer = 'Adam', learning_rate = lr, patience = np.random.randint(3, 10))
	return (mod, dt)

if __name__ == '__main__':
	
	# Parse command-line arguments
	path, model_dir, model1, model2, outputdir = vars(arg_parse()).values()	
	
	# Sample hyperparameters	
	n_linear, lr, gradients, pre_trained = random_hyperparameters()

	# Use random models if pre_trained
	if pre_trained == False:
		# Initialize an untrained model of kind == evolution
		name, n_out, n_conv, branch_type, conv_width1, conv_width2, conv_width3, _, patience, extra = model1.split('_')
		model1 = Evolution(input_dim = 192, n_out = int(n_out), n_conv = int(n_conv), branch_type = branch_type, conv_width = [int(conv_width1), int(conv_width2), int(conv_width3)], drop = 0.5)
		# Initialize an untrained model of kind == onesubclone
		name, n_out, n_conv, branch_type, conv_width1, conv_width2, conv_width3, _, extra = model2.split('_')
		model2 = OneSubclone(input_dim = 192, n_out = int(n_out), n_conv = int(n_conv), branch_type = branch_type, conv_width = [int(conv_width1), int(conv_width2), int(conv_width3)], drop = 0.5)
	else:
		# Load pre-trained models
		model1 = load_model(directory = model_dir, mod = model1, input_dim = 192, kind = 'evolution')
		model2 = load_model(directory = model_dir, mod = model2, input_dim = 192, kind = 'onesubclone')

	# Train models
	torch.manual_seed(123456)
	model, performance = train_temulator(pretrained_models = [model1, model2], n_linear = n_linear, lr = lr, gradients = gradients, directory = path, input_dim = 192, epochs = 4)
	
	# Annotate files
	params = "_".join([str(n_linear), str(lr), str(gradients), str(pre_trained)])
	model_id = ''.join([random.choice(string.ascii_uppercase + string.digits) for i in range(15)])
	
	# Save model and performance metrics
	performance.to_csv(outputdir + params + '.' + model_id + '.csv')	
	torch.save(model.state_dict(), outputdir + params + '.' + model_id + '.pt')